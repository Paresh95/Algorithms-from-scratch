{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Tutorial with a Simple Neural Network\n",
    "\n",
    "In this tutorial, we will implement a simple neural network with one hidden layer and demonstrate the backpropagation process using real numbers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "z = [0.5]\n",
      "a = [0.62245933]\n",
      "y = [-0.62245933]\n",
      "Loss L = [0.19372781]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Input data\n",
    "x = np.array([1.0])\n",
    "\n",
    "# Target\n",
    "t = np.array([0.0])\n",
    "\n",
    "# Initialise weights and biases\n",
    "W1 = np.array([0.5])\n",
    "b1 = np.array([0.0])\n",
    "W2 = np.array([-1.0])\n",
    "b2 = np.array([0.0])\n",
    "\n",
    "# Forward pass\n",
    "z = W1 * x + b1\n",
    "a = sigmoid(z)\n",
    "y = W2 * a + b2\n",
    "\n",
    "# Compute loss (Mean Squared Error)\n",
    "L = 0.5 * (y - t) ** 2\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(\"z =\", z)\n",
    "print(\"a =\", a)\n",
    "print(\"y =\", y)\n",
    "print(\"Loss L =\", L)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "In the forward pass, we compute the intermediate values for the hidden layer and the final output:\n",
    "\n",
    "$$\n",
    "z = W_1 \\cdot x + b_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "a = \\sigma(z) \\quad \\text{where} \\quad \\sigma \\text{ is the sigmoid function}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = W_2 \\cdot a + b_2\n",
    "$$\n",
    "\n",
    "We then compute the loss \\( L \\) using the mean squared error formula:\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{2} (y - t)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass:\n",
      "dL_dy = [-0.62245933]\n",
      "dL_dW2 = [-0.38745562]\n",
      "dL_db2 = [-0.62245933]\n",
      "dL_da = [0.62245933]\n",
      "dL_dW1 = [0.14628025]\n",
      "dL_db1 = [0.14628025]\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "# Gradient of loss w.r.t. output y\n",
    "dL_dy = y - t\n",
    "\n",
    "# Gradient of loss w.r.t. W2 and b2\n",
    "dy_dW2 = a\n",
    "dL_dW2 = dL_dy * dy_dW2\n",
    "dL_db2 = dL_dy\n",
    "\n",
    "# Gradient of loss w.r.t. activation a\n",
    "dy_da = W2\n",
    "dL_da = dL_dy * dy_da\n",
    "\n",
    "# Gradient of loss w.r.t. z\n",
    "da_dz = sigmoid_derivative(a)\n",
    "dz_dW1 = x\n",
    "dL_dW1 = dL_da * da_dz * dz_dW1\n",
    "dL_db1 = dL_da * da_dz\n",
    "\n",
    "print(\"Backward Pass:\")\n",
    "print(\"dL_dy =\", dL_dy)\n",
    "print(\"dL_dW2 =\", dL_dW2)\n",
    "print(\"dL_db2 =\", dL_db2)\n",
    "print(\"dL_da =\", dL_da)\n",
    "print(\"dL_dW1 =\", dL_dW1)\n",
    "print(\"dL_db1 =\", dL_db1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "\n",
    "In the backward pass, we use the chain rule to compute the gradients of the loss with respect to each weight and bias:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial y} = y - t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial y} \\cdot 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial a}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Parameters:\n",
      "W2 = [-0.99612544]\n",
      "b2 = [0.00622459]\n",
      "W1 = [0.4985372]\n",
      "b1 = [-0.0014628]\n"
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "eta = 0.01\n",
    "\n",
    "# Update weights and biases\n",
    "W2 -= eta * dL_dW2\n",
    "b2 -= eta * dL_db2\n",
    "W1 -= eta * dL_dW1\n",
    "b1 -= eta * dL_db1\n",
    "\n",
    "print(\"Updated Parameters:\")\n",
    "print(\"W2 =\", W2)\n",
    "print(\"b2 =\", b2)\n",
    "print(\"W1 =\", W1)\n",
    "print(\"b1 =\", b1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters\n",
    "\n",
    "Using the computed gradients, we update the weights and biases using gradient descent:\n",
    "\n",
    "$$\n",
    "W_2 \\leftarrow W_2 - \\eta \\cdot \\frac{\\partial L}{\\partial W_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_2 \\leftarrow b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_1 \\leftarrow W_1 - \\eta \\cdot \\frac{\\partial L}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_1 \\leftarrow b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}\n",
    "$$\n",
    "\n",
    "\n",
    "Where /eta is the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We have successfully demonstrated the forward and backward passes of a simple neural network with real numbers. The backpropagation process involves computing the gradients of the loss function with respect to each weight and bias using the chain rule, and then updating the parameters using gradient descent. This iterative process continues until the network converges to a minimum loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass:\n",
      "z = 0.5\n",
      "a = 0.622459352016449\n",
      "y = -0.622459352016449\n",
      "Loss L = 0.19372782111167908\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Input data\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# True target\n",
    "t = torch.tensor([0.0])\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = torch.tensor([0.5], requires_grad=True)\n",
    "b1 = torch.tensor([0.0], requires_grad=True)\n",
    "W2 = torch.tensor([-1.0], requires_grad=True)\n",
    "b2 = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = W1 * x + b1\n",
    "a = torch.sigmoid(z)\n",
    "y = W2 * a + b2\n",
    "\n",
    "# Compute loss (Mean Squared Error)\n",
    "L = 0.5 * (y - t) ** 2\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(\"z =\", z.item())\n",
    "print(\"a =\", a.item())\n",
    "print(\"y =\", y.item())\n",
    "print(\"Loss L =\", L.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Pass:\n",
      "dL_dW2 = -0.38745564222335815\n",
      "dL_db2 = -0.622459352016449\n",
      "dL_dW1 = 0.14628025889396667\n",
      "dL_db1 = 0.14628025889396667\n"
     ]
    }
   ],
   "source": [
    "# Backward pass\n",
    "L.backward()\n",
    "\n",
    "# Gradients\n",
    "# dL_dy = y.grad # will be None because it's not a leaf node\n",
    "dL_dW2 = W2.grad\n",
    "dL_db2 = b2.grad\n",
    "# dL_da = a.grad # will be None because it's not a leaf node\n",
    "dL_dW1 = W1.grad\n",
    "dL_db1 = b1.grad\n",
    "\n",
    "\n",
    "print(\"Backward Pass:\")\n",
    "# print(\"dL_dy =\", dL_dy)\n",
    "print(\"dL_dW2 =\", dL_dW2.item())\n",
    "print(\"dL_db2 =\", dL_db2.item())\n",
    "# print(\"dL_da =\", dL_da) \n",
    "print(\"dL_dW1 =\", dL_dW1.item())\n",
    "print(\"dL_db1 =\", dL_db1.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
